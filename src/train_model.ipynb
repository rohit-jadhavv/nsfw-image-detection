{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "data_dir = \"/Users/rohitjadhav/Desktop/Personal/nsfw-image-detection/clean_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 files belonging to 4 classes.\n",
      "Using 640 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  directory=data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 files belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hentai', 'neutral', 'porn', 'sexy']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 10:18:15.138572: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 10s 350ms/step - loss: 1.1640 - accuracy: 0.4453 - val_loss: 1.0123 - val_accuracy: 0.6500\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 9s 335ms/step - loss: 0.9933 - accuracy: 0.5625 - val_loss: 0.9784 - val_accuracy: 0.6250\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 10s 360ms/step - loss: 0.9280 - accuracy: 0.5656 - val_loss: 1.1409 - val_accuracy: 0.6125\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 9s 331ms/step - loss: 0.8515 - accuracy: 0.6109 - val_loss: 0.9036 - val_accuracy: 0.6500\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 11s 387ms/step - loss: 0.7348 - accuracy: 0.6703 - val_loss: 0.9797 - val_accuracy: 0.5188\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 10s 341ms/step - loss: 0.6279 - accuracy: 0.7219 - val_loss: 0.8221 - val_accuracy: 0.7125\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 10s 343ms/step - loss: 0.5666 - accuracy: 0.7172 - val_loss: 0.8029 - val_accuracy: 0.7188\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 10s 353ms/step - loss: 0.5261 - accuracy: 0.7437 - val_loss: 1.0438 - val_accuracy: 0.5188\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 11s 378ms/step - loss: 0.4838 - accuracy: 0.7703 - val_loss: 0.9311 - val_accuracy: 0.6750\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 10s 373ms/step - loss: 0.4769 - accuracy: 0.7641 - val_loss: 1.0477 - val_accuracy: 0.6687\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 49ms/step - loss: 1.0477 - accuracy: 0.6687\n",
      "Validation Accuracy: 66.87%\n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_accuracy = model.evaluate(val_ds)\n",
    "\n",
    "print(f'Validation Accuracy: {validation_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 21.  24.  35.]\n",
      "  [ 20.  23.  33.]\n",
      "  [ 22.  25.  36.]\n",
      "  ...\n",
      "  [ 74.  82.  98.]\n",
      "  [ 76.  85. 100.]\n",
      "  [ 44.  55.  70.]]\n",
      "\n",
      " [[ 17.  19.  30.]\n",
      "  [ 17.  19.  30.]\n",
      "  [ 18.  21.  31.]\n",
      "  ...\n",
      "  [ 47.  55.  73.]\n",
      "  [101. 110. 130.]\n",
      "  [ 61.  72.  89.]]\n",
      "\n",
      " [[ 16.  21.  33.]\n",
      "  [ 19.  22.  32.]\n",
      "  [ 18.  21.  31.]\n",
      "  ...\n",
      "  [ 51.  60.  80.]\n",
      "  [103. 111. 131.]\n",
      "  [ 68.  79.  96.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 12.  16.  28.]\n",
      "  [  9.  14.  26.]\n",
      "  [  7.  12.  24.]\n",
      "  ...\n",
      "  [ 17.  22.  34.]\n",
      "  [ 15.  20.  32.]\n",
      "  [ 15.  20.  32.]]\n",
      "\n",
      " [[  9.  14.  26.]\n",
      "  [  9.  14.  26.]\n",
      "  [  9.  14.  26.]\n",
      "  ...\n",
      "  [ 16.  21.  33.]\n",
      "  [ 12.  16.  28.]\n",
      "  [ 15.  20.  32.]]\n",
      "\n",
      " [[ 13.  18.  30.]\n",
      "  [ 10.  15.  27.]\n",
      "  [ 10.  15.  27.]\n",
      "  ...\n",
      "  [ 16.  21.  33.]\n",
      "  [ 10.  15.  27.]\n",
      "  [ 10.  15.  27.]]]\n",
      "tf.Tensor(\n",
      "[[[ 21.  24.  35.]\n",
      "  [ 20.  23.  33.]\n",
      "  [ 22.  25.  36.]\n",
      "  ...\n",
      "  [ 74.  82.  98.]\n",
      "  [ 76.  85. 100.]\n",
      "  [ 44.  55.  70.]]\n",
      "\n",
      " [[ 17.  19.  30.]\n",
      "  [ 17.  19.  30.]\n",
      "  [ 18.  21.  31.]\n",
      "  ...\n",
      "  [ 47.  55.  73.]\n",
      "  [101. 110. 130.]\n",
      "  [ 61.  72.  89.]]\n",
      "\n",
      " [[ 16.  21.  33.]\n",
      "  [ 19.  22.  32.]\n",
      "  [ 18.  21.  31.]\n",
      "  ...\n",
      "  [ 51.  60.  80.]\n",
      "  [103. 111. 131.]\n",
      "  [ 68.  79.  96.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 12.  16.  28.]\n",
      "  [  9.  14.  26.]\n",
      "  [  7.  12.  24.]\n",
      "  ...\n",
      "  [ 17.  22.  34.]\n",
      "  [ 15.  20.  32.]\n",
      "  [ 15.  20.  32.]]\n",
      "\n",
      " [[  9.  14.  26.]\n",
      "  [  9.  14.  26.]\n",
      "  [  9.  14.  26.]\n",
      "  ...\n",
      "  [ 16.  21.  33.]\n",
      "  [ 12.  16.  28.]\n",
      "  [ 15.  20.  32.]]\n",
      "\n",
      " [[ 13.  18.  30.]\n",
      "  [ 10.  15.  27.]\n",
      "  [ 10.  15.  27.]\n",
      "  ...\n",
      "  [ 16.  21.  33.]\n",
      "  [ 10.  15.  27.]\n",
      "  [ 10.  15.  27.]]], shape=(180, 180, 3), dtype=float32)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "[ 3.6518958  8.346274  -6.4045477 -0.2888847]\n",
      "[9.0620462e-03 9.9076146e-01 3.8883780e-07 1.7610309e-04]\n",
      "1\n",
      "Image: /Users/rohitjadhav/Desktop/Personal/nsfw_img_detctn/test_imgs/test2.webp, Predicted Class: neutral, Confidence: 99.08%\n",
      "Image: /Users/rohitjadhav/Desktop/Personal/nsfw_img_detctn/test_imgs/test1.jpeg, Predicted Class: neutral, Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "test_folder = \"/Users/rohitjadhav/Desktop/Personal/nsfw_img_detctn/test_imgs\"\n",
    "\n",
    "image_paths = [os.path.join(test_folder, file) for file in os.listdir(test_folder)]\n",
    "\n",
    "images = [tf.keras.utils.load_img(image, target_size=(img_width, img_height)) for image in image_paths]\n",
    "\n",
    "img_arrays = [tf.keras.utils.img_to_array(image) for image in images]\n",
    "\n",
    "img_tensor = tf.concat([tf.expand_dims(img, 0) for img in img_arrays], axis=0)\n",
    "\n",
    "predictions = model.predict(img_tensor)\n",
    "\n",
    "class_probabilities = tf.nn.softmax(predictions, axis=1).numpy()\n",
    "\n",
    "predicted_labels = np.argmax(class_probabilities, axis=1)\n",
    "\n",
    "print(predicted_labels[0])\n",
    "\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    class_name = class_names[predicted_labels[i]]\n",
    "    confidence = 100 * np.max(class_probabilities[i])\n",
    "    print(\"Image: {}, Predicted Class: {}, Confidence: {:.2f}%\".format(image_path, class_name, confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/nsfw_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('../models/nsfw_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x16b48dfd0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m sunflower_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com/url?sa=i&url=https\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m3A\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mthecinemaholic.com\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mupcoming-adult-anime\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124m&psig=AOvVaw2BdWUNz-_ZCOnb5lGr6a21&ust=1706010075604000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCNDUmbP18IMDFQAAAAAdAAAAABAI\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m sunflower_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple23\u001b[39m\u001b[38;5;124m'\u001b[39m, origin\u001b[38;5;241m=\u001b[39msunflower_url)\n\u001b[0;32m----> 4\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msunflower_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m img_array \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[1;32m      8\u001b[0m img_array \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Create a batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/utils/image_utils.py:423\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 423\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mpil_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be path-like or io.BytesIO, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/PIL/Image.py:3280\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3278\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3279\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3280\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x16b48dfd0>"
     ]
    }
   ],
   "source": [
    "sunflower_url = \"https://www.google.com/url?sa=i&url=https%3A%2F%2Fthecinemaholic.com%2Fupcoming-adult-anime%2F&psig=AOvVaw2BdWUNz-_ZCOnb5lGr6a21&ust=1706010075604000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCNDUmbP18IMDFQAAAAAdAAAAABAI\"\n",
    "sunflower_path = tf.keras.utils.get_file('apple23', origin=sunflower_url)\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    sunflower_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
